{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sacrifice_official.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dwnHSxvQ5YPb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import Categorical\n",
        "from typing import Dict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mse = nn.MSELoss()\n",
        "sf = nn.Softmax(dim=1)"
      ],
      "metadata": {
        "id": "tMNPOciY5d5a"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters"
      ],
      "metadata": {
        "id": "5tQKvRRS5gGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_of_agents = 2\n",
        "NUM_OF_ACTIONS = 2  # 3 if with mediator, 2 if solo \n",
        "MEDIATOR_ACTION = 3\n",
        "\n",
        "hidden_size = 16\n",
        "\n",
        "lr_actor = 1e-3\n",
        "lr_critic = 1e-3\n",
        "lr_lambda = 1e-3\n",
        "\n",
        "NUM_OF_SEEDS = 50\n",
        "IC_CONSTRAINT = True # False if Naive mediator, True if IC constrined mediator\n",
        "IC_COEF = 2"
      ],
      "metadata": {
        "id": "PEuw2ACp5iXe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logging function"
      ],
      "metadata": {
        "id": "miDqzyG19ZBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_empty_dict():\n",
        "  \"\"\"\n",
        "  Make dictionary for further logging.\n",
        "  \"\"\"\n",
        "  prefix = [\"med_agent\", \"med_agent\", \"agent\"]\n",
        "  suffix = [\"solo\", \"full\", \"solo\"]\n",
        "  size = [NUM_OF_ACTIONS, NUM_OF_ACTIONS, NUM_OF_ACTIONS + 1]\n",
        "  \n",
        "  prob_dist = {}\n",
        "  for p, s, sz in zip(prefix, suffix, size):\n",
        "    for i in range(num_of_agents):\n",
        "      prob_dist[p + f\"_{i}_\" + s] = torch.empty((NUM_OF_SEEDS, sz))\n",
        "\n",
        "  return prob_dist"
      ],
      "metadata": {
        "id": "Nc5S0MVWPRpE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def log_pd(dictionary, n: int, med_actor, ag_actors):\n",
        "  \"\"\"\n",
        "  Saves policies of agents and mediator to dictionary.\n",
        "  \"\"\"\n",
        "  test_0 = torch.tensor([[1., 0., 0.]])\n",
        "  test_1 = torch.tensor([[0., 1., 1.]])\n",
        "  testf_0 = torch.tensor([[1., 1., 0.]])\n",
        "  testf_1 = torch.tensor([[1., 1., 1.]])\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for i, actor in enumerate(ag_actors):\n",
        "      dictionary[f\"agent_{i}_solo\"][n] = sf(actor(torch.zeros((1, 1)))).squeeze(0)\n",
        "\n",
        "    if MEDIATOR_ACTION == NUM_OF_ACTIONS:\n",
        "      dictionary[\"med_agent_0_solo\"][n] = sf(med_actor(test_0)).squeeze(0)\n",
        "      dictionary[\"med_agent_1_solo\"][n] = sf(med_actor(test_1)).squeeze(0)\n",
        "      dictionary[\"med_agent_0_full\"][n] = sf(med_actor(testf_0)).squeeze(0)\n",
        "      dictionary[\"med_agent_1_full\"][n] = sf(med_actor(testf_1)).squeeze(0)"
      ],
      "metadata": {
        "id": "fm5e4wpCPl6o"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sacrifice table"
      ],
      "metadata": {
        "id": "EQ2O9NNNNSU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reward_table = torch.tensor([[1, 1], [3, 0], [5, 0],\n",
        "                             [0, 3], [2, 2], [5, 0],\n",
        "                             [0, 0], [0, 0], [0, 0]]).float()\n",
        "table_width = 3"
      ],
      "metadata": {
        "id": "krjq0MbF-ahw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train function"
      ],
      "metadata": {
        "id": "eZQvNai_5tPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(n: int, prob_dist: Dict[str, torch.Tensor]):\n",
        "  entropy_coef_start = 5e-1\n",
        "  entropy_coef = entropy_coef_start\n",
        "  entropy_coef_finish = 1e-2\n",
        "  entropy_coef_step = 1 / ((entropy_coef_start / entropy_coef_finish) ** (1 / n_episodes))\n",
        "\n",
        "  log_lambda_IC = torch.zeros(num_of_agents)\n",
        "\n",
        "  torch.manual_seed(n)\n",
        "\n",
        "  # Init agents\n",
        "  agents_actors = [nn.Linear(1, NUM_OF_ACTIONS + 1) for _ in range(num_of_agents)]\n",
        "  agents_critic = [nn.Linear(1, 1) for _ in range(num_of_agents)]\n",
        "\n",
        "  # Init mediator\n",
        "  mediator_actor = nn.Sequential(nn.Linear(num_of_agents + 1, hidden_size), nn.ReLU(), nn.Linear(hidden_size, NUM_OF_ACTIONS))\n",
        "  mediator_critic = nn.Sequential(nn.Linear(num_of_agents, 2 * hidden_size), nn.ReLU(), nn.Linear(2 * hidden_size, num_of_agents))\n",
        "\n",
        "  # Agents optimizers\n",
        "  opts_actor = [torch.optim.Adam(actor.parameters(), lr_actor, weight_decay=1e-3) for actor in agents_actors]\n",
        "  opts_critic = [torch.optim.Adam(critic.parameters(), lr_critic, weight_decay=1e-3) for critic in agents_critic]\n",
        "  \n",
        "  # Mediator optimizers\n",
        "  opt_mediator_actor = torch.optim.Adam(mediator_actor.parameters(), lr_actor, weight_decay=1e-3)\n",
        "  opt_mediator_critic = torch.optim.Adam(mediator_critic.parameters(), lr_critic, weight_decay=1e-3)\n",
        "  \n",
        "  # Training\n",
        "  dummy_state = torch.zeros((batch_size, 1)) \n",
        "\n",
        "  for episode in range(n_episodes):\n",
        "    act = torch.zeros(batch_size, num_of_agents).long() \n",
        "\n",
        "    # Agents act\n",
        "    with torch.no_grad():\n",
        "      for i, actor in enumerate(agents_actors):\n",
        "        l = actor(dummy_state)\n",
        "        C = Categorical(logits=l)\n",
        "        act[:, i] = C.sample()\n",
        "\n",
        "    # Who is in coalition?\n",
        "    coalition = torch.zeros_like(act, dtype=torch.float)\n",
        "    coal_1, coal_2 = torch.where(act == MEDIATOR_ACTION)\n",
        "    coalition[coal_1, coal_2] = 1\n",
        "\n",
        "    coal_state = torch.cat([coalition[coal_1], coal_2.unsqueeze(1)], dim=1)\n",
        "\n",
        "    act_final = torch.clone(act)\n",
        "    \n",
        "    if coal_1.numel():\n",
        "      # Take action for coalition\n",
        "      with torch.no_grad():\n",
        "        mediator_logit = mediator_actor(coal_state)\n",
        "      mediator_C = Categorical(logits=mediator_logit)\n",
        "\n",
        "      # Swap \"mediator\" action on picked action\n",
        "      act_final[coal_1, coal_2] = mediator_C.sample()\n",
        "\n",
        "    # Compute reward\n",
        "    reward_ids = act_final[:, 1] + act_final[:, 0] * table_width\n",
        "    reward_ids = reward_ids.view(-1).long()\n",
        "    reward = reward_table[reward_ids]\n",
        "\n",
        "    # Actor update\n",
        "    for critic, actor, opt_critic, opt_actor, r, action in zip(agents_critic, agents_actors, opts_critic, opts_actor, reward.T, act.T):\n",
        "      V = critic(dummy_state).squeeze(1)\n",
        "      adv_pg = r - V.detach()\n",
        "      adv = mse(r, V)\n",
        "      logits = actor(dummy_state)\n",
        "      C = Categorical(logits=logits)\n",
        "\n",
        "      pg_loss = - adv_pg * C.log_prob(action)\n",
        "      pg_loss = pg_loss.mean() - entropy_coef * C.entropy().mean()\n",
        "\n",
        "      opt_critic.zero_grad()\n",
        "      opt_actor.zero_grad()\n",
        "\n",
        "      adv.backward()\n",
        "      pg_loss.backward()\n",
        "\n",
        "      opt_actor.step()\n",
        "      opt_critic.step()\n",
        "    \n",
        "    # Critic loss\n",
        "    adv_total = mse(reward, mediator_critic(coalition))\n",
        "    if coal_1.numel():\n",
        "\n",
        "      # Sum of rewards for each coalition\n",
        "      total_reward = reward * coalition\n",
        "      total_reward = total_reward.sum(dim=1)\n",
        "\n",
        "      # Computing policy gradients\n",
        "      mediator_logit = mediator_actor(coal_state)\n",
        "      mediator_C = Categorical(logits=mediator_logit)\n",
        "\n",
        "      tot_rew = total_reward[coal_1].view(-1, 1)\n",
        "      \n",
        "      with torch.no_grad():\n",
        "        tot_adv = tot_rew.view(-1) - (mediator_critic(coalition) * coalition).sum(dim=1)[coal_1]\n",
        " \n",
        "        if IC_CONSTRAINT:\n",
        "          mask = 1 < torch.sum(coalition, dim=1) # masking on coalitions with more than one participant\n",
        "\n",
        "          for idx in range(num_of_agents):\n",
        "            if mask.any():\n",
        "              counterfactual_coal = torch.clone(coalition[mask])\n",
        "              counterfactual_coal[:, idx] = 0\n",
        "              ic_adv = reward[mask, idx] - mediator_critic(counterfactual_coal)[:, idx]\n",
        "\n",
        "              new_ids = torch.logical_and(torch.sum(coalition[coal_1], dim=1) == num_of_agents, coal_2 == idx)\n",
        "              tot_adv[new_ids] += log_lambda_IC[idx].exp() * ic_adv\n",
        "\n",
        "              # Weighting\n",
        "              ic_adv[ic_adv < 0] *= IC_COEF   \n",
        "              \n",
        "              # Lambda update\n",
        "              log_lambda_IC[idx] -= lr_lambda * ic_adv.mean()\n",
        "  \n",
        "          log_lambda_IC = torch.clip(log_lambda_IC, min=-4, max=4)\n",
        "\n",
        "      # Actor loss\n",
        "      pg_total = - tot_adv.flatten() * mediator_C.log_prob(act_final[coal_1, coal_2])\n",
        "      pg_total = pg_total.mean() - entropy_coef * mediator_C.entropy().mean()\n",
        "\n",
        "      # Update mediator actor\n",
        "      opt_mediator_actor.zero_grad()\n",
        "      pg_total.backward()\n",
        "      torch.nn.utils.clip_grad_value_(mediator_actor.parameters(), 0.1)\n",
        "      opt_mediator_actor.step()\n",
        "\n",
        "    # Update mediator critic  \n",
        "    opt_mediator_critic.zero_grad()\n",
        "    adv_total.backward()\n",
        "    torch.nn.utils.clip_grad_value_(mediator_critic.parameters(), 0.1)\n",
        "    opt_mediator_critic.step()\n",
        "\n",
        "    # schedule entropy\n",
        "    entropy_coef = max(entropy_coef * entropy_coef_step, entropy_coef_finish)\n",
        "\n",
        "  log_pd(prob_dist, n, mediator_actor, agents_actors)"
      ],
      "metadata": {
        "id": "Ri9ICnSD5yK6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiments"
      ],
      "metadata": {
        "id": "OAov4pWUNV6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "n_episodes = 10_000"
      ],
      "metadata": {
        "id": "42NBxhfH5vw_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "experiment_configs =  [('no_mediator', 2, False), ('naive', 3, False), ('ic_mediator', 3, True)]"
      ],
      "metadata": {
        "id": "nq45NJcSH_R2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for conf in experiment_configs:\n",
        "  name, NUM_OF_ACTIONS, IC_CONSTRAINT = conf\n",
        "  prob_dist = make_empty_dict()\n",
        "  for i in range(NUM_OF_SEEDS):\n",
        "    train(i, prob_dist)\n",
        "  torch.save(prob_dist, f\"./probs_{name}.pt\")"
      ],
      "metadata": {
        "id": "PDc3UEoOIhdW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}