{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yB6HBafZ-zHE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import Categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ICJZ-I0KW2wk"
      },
      "outputs": [],
      "source": [
        "mse = nn.MSELoss()\n",
        "sf = nn.Softmax(dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZHSwn_B--Js"
      },
      "source": [
        "### Initialize agents and mediator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lwSzOb-QENOR"
      },
      "outputs": [],
      "source": [
        "num_of_actions = 2\n",
        "MEDIATOR_ACTION = 2\n",
        "hidden_size = 16\n",
        "lr_actor = 1e-3\n",
        "lr_critic = 1e-2\n",
        "lr_lambda = 1e-2\n",
        "\n",
        "NUM_OF_SEEDS = 10\n",
        "num_of_agents = 3\n",
        "PG_MULTIPLIER = 5 if num_of_agents > 10 else 2\n",
        "NORM_CONST = 1 / (PG_MULTIPLIER - 1)\n",
        "\n",
        "P_CONSTRAINT = True\n",
        "P_COEF = 0.99"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCBFK-xmV1N1"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "w4SMRPD2WqlP"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "n_episodes = 20_000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XAi3ntDbV29P"
      },
      "outputs": [],
      "source": [
        "def train(n:int, π_m, R, π_a, π_total):\n",
        "\n",
        "  class NormalizingLayer(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x / num_of_agents * 2 - 1\n",
        "\n",
        "  torch.manual_seed(n)\n",
        "\n",
        "  # Agents nets\n",
        "  agents_actors = [nn.Linear(1, num_of_actions + 1) for _ in range(num_of_agents)]\n",
        "  agents_critic = [nn.Linear(1, 1) for _ in range(num_of_agents)]\n",
        "  \n",
        "  # Optimizers (agents)\n",
        "  opts_actor = [torch.optim.Adam(actor.parameters(), lr_actor, weight_decay=1e-3) for actor in agents_actors]\n",
        "  opts_critic = [torch.optim.Adam(critic.parameters(), lr_critic, weight_decay=1e-3) for critic in agents_critic]\n",
        "\n",
        "  # Mediator nets\n",
        "  mediator_actor = nn.Sequential(NormalizingLayer(), nn.Linear(1, hidden_size), nn.ReLU(), nn.Linear(hidden_size, num_of_actions))\n",
        "  mediator_critic = nn.Sequential(NormalizingLayer(), nn.Linear(1, 2 * hidden_size), nn.ReLU(), nn.Linear(2 * hidden_size, 1))\n",
        "\n",
        "  # Optimizers (mediator)\n",
        "  opt_mediator_actor = torch.optim.Adam(mediator_actor.parameters(), lr_actor, weight_decay=1e-3)\n",
        "  opt_mediator_critic = torch.optim.Adam(mediator_critic.parameters(), lr_critic, weight_decay=1e-3)\n",
        "\n",
        "  # Hyperparameters\n",
        "  entropy_coef_start = 0.1\n",
        "  entropy_coef = entropy_coef_start\n",
        "  entropy_coef_finish = 0.001\n",
        "  entropy_coef_step = 1 / ((entropy_coef_start / entropy_coef_finish) ** (1 / n_episodes))\n",
        "\n",
        "  log_lambda_P = torch.zeros(1)\n",
        "\n",
        "  # Training \n",
        "  dummy_state = torch.zeros((batch_size, 1)) \n",
        "\n",
        "  for episode in range(n_episodes):\n",
        "    act = torch.empty((batch_size, num_of_agents)) \n",
        "\n",
        "    # Agents act\n",
        "    for i, actor in enumerate(agents_actors):\n",
        "      with torch.no_grad():\n",
        "        l = actor(dummy_state)\n",
        "      C = Categorical(logits=l)\n",
        "      act[:, i] = C.sample()\n",
        "\n",
        "    # Who is in coalition?\n",
        "    coalition = torch.zeros((batch_size, num_of_agents))\n",
        "    coal_1, coal_2 = torch.where(act == MEDIATOR_ACTION)\n",
        "    coalition[coal_1, coal_2] = 1\n",
        "\n",
        "    coal_sum = torch.sum(coalition, dim=1, keepdim=True)\n",
        "    coal_state = coal_sum[coal_1].view(-1, 1)\n",
        "\n",
        "    # Take action for coalition\n",
        "    with torch.no_grad():\n",
        "      mediator_logit = mediator_actor(coal_state)\n",
        "    mediator_C = Categorical(logits=mediator_logit)\n",
        "\n",
        "    # Swap \"mediator\" action on picked action\n",
        "    act_final = torch.clone(act)\n",
        "    act_final[coal_1, coal_2] = mediator_C.sample().float()\n",
        "\n",
        "    # Compute reward\n",
        "    reward = (- act_final + torch.sum(act_final, dim=1, keepdim=True) * (PG_MULTIPLIER / num_of_agents)) * NORM_CONST  # maximum reward is 1\n",
        "\n",
        "    # Actor update\n",
        "    for critic, actor, opt_critic, opt_actor, r, action in zip(agents_critic, agents_actors, opts_critic, opts_actor, reward.T, act.T):\n",
        "      V = critic(dummy_state).squeeze(1)\n",
        "      adv_pg = r - V.detach()\n",
        "      adv = mse(r, V)\n",
        "      logits = actor(dummy_state)\n",
        "      C = Categorical(logits=logits)\n",
        "\n",
        "      pg_loss = - adv_pg * C.log_prob(action)\n",
        "      pg_loss = pg_loss.mean() - entropy_coef * C.entropy().mean()\n",
        "\n",
        "      opt_critic.zero_grad()\n",
        "      opt_actor.zero_grad()\n",
        "\n",
        "      adv.backward()\n",
        "      pg_loss.backward()\n",
        "\n",
        "      opt_actor.step()\n",
        "      opt_critic.step()\n",
        "\n",
        "    # Compute sum of rewards of coalition\n",
        "    total_reward = reward * coalition\n",
        "    total_reward = total_reward.sum(dim=1)\n",
        "    uniq_id_1 = torch.unique(coal_1)\n",
        "\n",
        "    # Critic loss\n",
        "    R_total = total_reward[uniq_id_1].flatten()\n",
        "    V_total = mediator_critic(coal_sum[uniq_id_1]).flatten()\n",
        "    adv_total = mse(V_total, R_total)\n",
        "\n",
        "\n",
        "    # Log probabilities for policy gradients\n",
        "    mediator_logit = mediator_actor(coal_state)\n",
        "    mediator_C = Categorical(logits=mediator_logit)\n",
        "\n",
        "    tot_rew = total_reward[coal_1].view(-1, 1)\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "      tot_adv = tot_rew - mediator_critic(coal_state)\n",
        "\n",
        "      if P_CONSTRAINT:\n",
        "        mask = (1 < coal_state) & (coal_state < num_of_agents)\n",
        "\n",
        "        p_reward = reward * (1 - coalition)\n",
        "        pun_adv = p_reward.sum(dim=1)[coal_1].view(-1, 1)[mask] / (num_of_agents - coal_sum)[coal_1].view(-1, 1)[mask]\n",
        "        pun_adv -= mediator_critic(coal_state + 1)[mask] / (coal_state[mask] + 1)\n",
        "\n",
        "        tot_adv[mask] -= log_lambda_P.exp() * pun_adv\n",
        "\n",
        "        # update lambda\n",
        "        if pun_adv.shape[0] > 1:\n",
        "          pun_adv[pun_adv < 0] *= P_COEF\n",
        "          log_lambda_P += lr_lambda * pun_adv.mean()\n",
        "        log_lambda_P = torch.clip(log_lambda_P, min=-4, max=4)\n",
        "\n",
        "    # Actor's loss\n",
        "    pg_total = - tot_adv.flatten() * mediator_C.log_prob(act_final[coal_1, coal_2])\n",
        "    pg_total = pg_total.mean() - entropy_coef * mediator_C.entropy().mean()\n",
        "\n",
        "\n",
        "    # Backward\n",
        "    opt_mediator_actor.zero_grad()\n",
        "    opt_mediator_critic.zero_grad()\n",
        "\n",
        "    adv_total.backward()\n",
        "    pg_total.backward()\n",
        "\n",
        "    torch.nn.utils.clip_grad_value_(mediator_actor.parameters(), 0.1)\n",
        "    torch.nn.utils.clip_grad_value_(mediator_critic.parameters(), 0.1)\n",
        "\n",
        "    opt_mediator_actor.step()\n",
        "    opt_mediator_critic.step()\n",
        "\n",
        "    # schedule entropy\n",
        "    entropy_coef = max(entropy_coef * entropy_coef_step, entropy_coef_finish)\n",
        "\n",
        "  # Kind of logging\n",
        "  R.append(reward.mean().item())\n",
        "\n",
        "  with torch.no_grad():\n",
        "    π_s = torch.empty(num_of_agents, num_of_actions + 1)\n",
        "    for i, actor in enumerate(agents_actors):\n",
        "      π_s[i] = sf(actor(torch.zeros(1, 1)))\n",
        "    π_a.append(π_s.mean(dim=0)[2])\n",
        "\n",
        "  π_m.append(act_final[coal_1, coal_2].mean())\n",
        "\n",
        "  test_state = torch.arange(1, num_of_agents + 1).unsqueeze(1).float()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    π_total.append(sf(mediator_actor(test_state.float())))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving"
      ],
      "metadata": {
        "id": "WR6q_xWfQ_Ok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_dict(pi_med, mean_rew, pi_agents, pi_total, filename):\n",
        "  result_dict = {}\n",
        "  result_dict[\"mediator_c\"] = pi_med\n",
        "  result_dict[\"mean_reward\"] = mean_rew\n",
        "  result_dict[\"prob_m\"] = pi_agents\n",
        "  result_dict['policies'] = pi_total\n",
        "\n",
        "  torch.save(result_dict, filename)"
      ],
      "metadata": {
        "id": "shDYBp02tv8J"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run for all seeds"
      ],
      "metadata": {
        "id": "IYGHbIHDKJCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "experiment_configs = [('3_naive', 3, False), ('3_p_constraint', 3, True), ('10_p_constraint', 10, True), ('25_p_constraint', 25, True)]"
      ],
      "metadata": {
        "id": "uaU_pTCyR_Zd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for conf in experiment_configs:\n",
        "  name, num_of_agents, P_CONSTRAINT = conf\n",
        "  PG_MULTIPLIER = 5 if num_of_agents > 10 else 2\n",
        "  NORM_CONST = 1 / (PG_MULTIPLIER - 1)\n",
        "\n",
        "  pi_med = []\n",
        "  mean_rew = []\n",
        "  pi_agents = []\n",
        "  pi_total = []\n",
        "\n",
        "  for i in range(NUM_OF_SEEDS):\n",
        "    train(i, pi_med, mean_rew, pi_agents, pi_total)\n",
        "    print(f\"run {i + 1}\")\n",
        "\n",
        "  save_dict(pi_med, mean_rew, pi_agents, pi_total, f'./probs_{name}.pt')"
      ],
      "metadata": {
        "id": "mGo-PGr5fAd0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "PGG_official.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}